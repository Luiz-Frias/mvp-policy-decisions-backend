#!/bin/bash
# ğŸ›¡ï¸ MASTER RULESET ENFORCEMENT - Pre-commit Validation Pipeline
# Enforces Defensive Programming & Performance First Principles

set -euo pipefail

# Timeout configuration to prevent git locks
TIMEOUT_SECONDS=300  # 5 minutes max for all checks

# If not already running under timeout, re-exec the script within timeout to avoid git locks.
if [ -z "${INSIDE_TIMEOUT:-}" ]; then
    export INSIDE_TIMEOUT=1
    exec timeout --preserve-status $TIMEOUT_SECONDS "$0" "$@"
fi

# -----------------------------------------------------------------------------

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

print_status() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }
print_header() { echo -e "${PURPLE}$1${NC}"; }

print_header "ğŸ›¡ï¸ MASTER RULESET ENFORCEMENT - Pre-commit Validation"
print_status "Enforcing Defensive Programming & Performance First Principles..."

# ---------------------------------------------
# MASTER RULE: Pydantic / Defensive checks
# We want to collect *all* violations and then exit once, rather than aborting
# at the first failure.  Temporarily relax `set -e` and aggregate results.
# ---------------------------------------------

print_status "ğŸ” MASTER RULE: Validating Pydantic model compliance..."

# Disable immediate-exit so we can gather every issue.
set +e
VIOLATIONS=0

# 1ï¸âƒ£  Dictionary usage analysis - SYSTEM_BOUNDARY classification
ALL_DICT_FILES=$(find src -name "*.py" -not -path "*/test*" -exec grep -l "dict\\[" {} \; 2>/dev/null)

if [ -n "$ALL_DICT_FILES" ]; then
    print_status "ğŸ” Analyzing dictionary usage patterns..."
    
    while IFS= read -r file; do
        if [ -n "$file" ]; then
            if grep -q "SYSTEM_BOUNDARY" "$file" 2>/dev/null; then
                print_status "===NOTICE=== ğŸ“ ${file} (infrastructure layer - flexible dict usage approved)"
            else
                print_error "âŒ BUSINESS LOGIC VIOLATION: ${file} uses dict[str, Any] without SYSTEM_BOUNDARY annotation"
                print_status "ğŸ’¡ Add '# SYSTEM_BOUNDARY: <reason>' comment if this is infrastructure code"
                VIOLATIONS=$((VIOLATIONS+1))
            fi
        fi
    done <<< "$ALL_DICT_FILES"
fi

# 2ï¸âƒ£  Pydantic models missing frozen=True (exclude tests)
BASEMODEL_FILES=$(find src -name "*.py" -not -path "*/test*" -exec grep -l "class.*BaseModel" {} \; 2>/dev/null)
UNFROZEN_LIST=""
if [ -n "$BASEMODEL_FILES" ]; then
    UNFROZEN_LIST=$(echo "$BASEMODEL_FILES" | xargs grep -L "frozen.*=.*True" 2>/dev/null)
fi
if [ -n "$UNFROZEN_LIST" ]; then
    COUNT=$(echo "$UNFROZEN_LIST" | wc -l)
    print_error "âŒ MASTER RULE VIOLATION: $COUNT model(s) lack frozen=True:"
    echo "$UNFROZEN_LIST" | sed 's/^/   â€¢ /'
    VIOLATIONS=$((VIOLATIONS+COUNT))
fi

# 3ï¸âƒ£  Public functions missing @beartype (exclude tests)
PUBLIC_FUNCS_FILES=$(find src -name "*.py" -not -path "*/test*" -exec grep -l "^def [^_]" {} \; 2>/dev/null)
MISSING_BEAR=""
if [ -n "$PUBLIC_FUNCS_FILES" ]; then
    MISSING_BEAR=$(echo "$PUBLIC_FUNCS_FILES" | xargs grep -L "@beartype" 2>/dev/null)
fi
if [ -n "$MISSING_BEAR" ]; then
    COUNT=$(echo "$MISSING_BEAR" | wc -l)
    print_error "âŒ MASTER RULE VIOLATION: $COUNT public function file(s) without @beartype:"
    echo "$MISSING_BEAR" | sed 's/^/   â€¢ /'
    VIOLATIONS=$((VIOLATIONS+COUNT))
fi

# Re-enable strict mode for the remainder of the script.
set -e

# Final decision for this validation block
if [ "$VIOLATIONS" -gt 0 ]; then
    print_error "ğŸš¨ MASTER RULESET: Fix the violations above before committing"
    exit 1
else
    print_success "âœ… Pydantic model compliance verified"
fi

# 2. MANDATORY: MyPy Strict Mode Enforcement
print_status "ğŸ”’ MASTER RULE: MyPy strict mode validation..."
if ! uv run mypy --strict src; then
    print_error "âŒ MASTER RULE VIOLATION: MyPy strict mode failed"
    print_error "ğŸš¨ TYPE SAFETY: Code must pass mypy --strict without ignores"
    exit 1
fi
print_success "âœ… MyPy strict mode validation passed"

# 3. MANDATORY: Performance Benchmark Validation
print_status "âš¡ MASTER RULE: Performance benchmark validation..."
LONG_FUNCTIONS=$(find src -name "*.py" -exec grep -c "^def \|^    def " {} \; 2>/dev/null | \
    awk -F: "{if (\$2 > 10) print \$1}" | wc -l)
BENCHMARK_FILES=$(find tests -name "*benchmark*" -o -name "*perf*" 2>/dev/null | wc -l)
if [ "$LONG_FUNCTIONS" -gt 0 ] && [ "$BENCHMARK_FILES" -eq 0 ]; then
    print_warning "âš ï¸ PERFORMANCE RULE: Functions >10 lines should have performance benchmarks"
    print_status "ğŸ’¡ Create tests/benchmarks/ with pytest-benchmark tests"
fi

# 4. Pre-commit hooks (fast file-level checks)
print_status "ğŸ“ Running pre-commit hooks (auto-fixing issues)..."
# Run pre-commit hooks up to 3 times to handle auto-fixes
max_attempts=3
attempt=1
hooks_passed=false

while [ $attempt -le $max_attempts ]; do
    print_status "ğŸ”§ Pre-commit run attempt $attempt/$max_attempts..."

    if uv run pre-commit run --all-files; then
        hooks_passed=true
        print_success "âœ… All pre-commit hooks passed!"
        break
    else
        if [ $attempt -lt $max_attempts ]; then
            print_warning "âš¡ Pre-commit hooks auto-fixed issues, running again..."

            # Auto-stage any files that were modified by the hooks
            if ! git diff --quiet; then
                print_status "ğŸ“ Auto-staging fixed files for next attempt..."
                git add -A
                print_success "âœ… Modified files staged for retry"
            fi

            # Give a moment for file system to sync
            sleep 1
        fi
    fi

    ((attempt++))
done

if [ "$hooks_passed" != "true" ]; then
    print_error "âŒ Pre-commit hooks failed after $max_attempts attempts!"
    print_error "ğŸš¨ MASTER RULESET VIOLATION: Code quality standards not met"
    print_status "ğŸ’¡ Review the errors above and fix manually, then run:"
    print_status "   cd $(pwd) && pre-commit run --all-files"
    exit 1
fi

# Verify all changes have been staged after pre-commit hooks
if ! git diff --quiet; then
    print_status "ğŸ“ Staging any remaining auto-fixed changes..."
    git add -A
    print_success "âœ… Final auto-fixes applied and staged"
fi

# 5. MANDATORY: Ruff ultra-fast linting
print_status "ğŸš€ MASTER RULE: Ruff ultra-fast linting..."
if ! uv run ruff check src tests; then
    print_error "âŒ MASTER RULE VIOLATION: Ruff linting failed"
    exit 1
fi

# Run ruff format and stage any additional changes
uv run ruff format src tests
if ! git diff --quiet; then
    print_status "ğŸ“ Auto-staging ruff format changes..."
    git add -A
    print_success "âœ… Ruff formatting changes staged"
fi

# Now check that everything is properly formatted
if ! uv run ruff format --check src tests; then
    print_error "âŒ MASTER RULE VIOLATION: Code formatting issues found"
    exit 1
fi
print_success "âœ… Ruff linting and formatting verified"

# 6. MANDATORY: Security vulnerability scanning
print_status "ğŸ›¡ï¸ MASTER RULE: Security vulnerability scanning..."

# Bandit security analysis
uv run bandit -r src -f json -o bandit-report.json >/dev/null 2>&1 || true
if [ -f "bandit-report.json" ]; then
    HIGH_ISSUES=$(cat bandit-report.json 2>/dev/null | jq -r '.results[] | select(.issue_severity == "HIGH") | .test_name' 2>/dev/null | wc -l || echo "0")
    MEDIUM_ISSUES=$(cat bandit-report.json 2>/dev/null | jq -r '.results[] | select(.issue_severity == "MEDIUM") | .test_name' 2>/dev/null | wc -l || echo "0")
    LOW_ISSUES=$(cat bandit-report.json 2>/dev/null | jq -r '.results[] | select(.issue_severity == "LOW") | .test_name' 2>/dev/null | wc -l || echo "0")
    
    if [ "$HIGH_ISSUES" -gt 0 ]; then
        print_error "âŒ MASTER RULE VIOLATION: $HIGH_ISSUES high-severity security issues found"
        print_error "ğŸš¨ SECURITY FIRST: Fix all high-severity issues before commit"
        print_status "ğŸ’¡ Run 'uv run bandit -r src' for detailed security analysis"
        exit 1
    elif [ "$MEDIUM_ISSUES" -gt 0 ] || [ "$LOW_ISSUES" -gt 0 ]; then
        print_warning "âš ï¸ SECURITY AWARENESS: $MEDIUM_ISSUES medium, $LOW_ISSUES low severity issues found"
        print_status "ğŸ’¡ Review security recommendations with 'uv run bandit -r src'"
    else
        print_success "âœ… No security vulnerabilities detected"
    fi
    
    # Clean up report
    rm -f bandit-report.json
else
    print_warning "âš ï¸ Bandit security scan failed - install with 'uv add --dev bandit'"
fi

# Dependency vulnerability check
print_status "ğŸ” Checking dependency vulnerabilities..."
if command -v safety >/dev/null 2>&1 || uv run python -c "import safety" >/dev/null 2>&1; then
    if ! uv run safety check --json >/dev/null 2>&1; then
        print_warning "âš ï¸ SECURITY: Dependency vulnerabilities found"
        print_status "ğŸ’¡ Run 'uv run safety check' for detailed vulnerability report"
    else
        print_success "âœ… No known dependency vulnerabilities"
    fi
else
    print_status "ğŸ“¦ Installing safety for dependency scanning..."
    uv add --dev safety >/dev/null 2>&1 || true
fi

# 7. MANDATORY: Memory leak detection in tests
print_status "ğŸ§  MASTER RULE: Memory leak detection..."
if [ -d "tests" ]; then
    # Skip benchmark tests during pre-commit (Wave 3 optimization)
    if ! uv run pytest tests/unit --maxfail=1 -q --tb=no -m "not benchmark" 2>/dev/null; then
        print_error "âŒ MASTER RULE VIOLATION: Unit tests failed"
        print_status "ğŸ’¡ If benchmark plugin is missing, install with: uv add --dev pytest-benchmark"
        exit 1
    fi

    # Check for memory tests
    MEMORY_TESTS=$(find tests -name "*.py" -exec grep -l "tracemalloc\|memray\|memory" {} \; 2>/dev/null | wc -l)
    if [ "$MEMORY_TESTS" -eq 0 ]; then
        print_warning "âš ï¸ PERFORMANCE RULE: No memory tests found"
        print_status "ğŸ’¡ Add memory leak detection tests with tracemalloc"
    fi
fi

# 8. Code complexity and maintainability analysis
print_status "ğŸ§® MASTER RULE: Code complexity and maintainability analysis..."

# Check for overly complex functions using radon
if command -v radon >/dev/null 2>&1 || uv run python -c "import radon" >/dev/null 2>&1; then
    COMPLEX_FUNCTIONS=$(uv run radon cc src -s | grep -E "F|E|D" | wc -l || echo "0")
    if [ "$COMPLEX_FUNCTIONS" -gt 0 ]; then
        print_warning "âš ï¸ MAINTAINABILITY: $COMPLEX_FUNCTIONS high-complexity functions found"
        print_status "ğŸ’¡ Consider refactoring functions with complexity grade D, E, or F"
        print_status "   Run 'uv run radon cc src -s' for detailed analysis"
    else
        print_success "âœ… Code complexity within acceptable bounds"
    fi
else
    print_status "ğŸ“Š Installing radon for complexity analysis..."
    uv add --dev radon >/dev/null 2>&1 || true
fi

# Check maintainability index
if command -v radon >/dev/null 2>&1 || uv run python -c "import radon" >/dev/null 2>&1; then
    LOW_MAINTAIN=$(uv run radon mi src -s | grep -E "C|D|F" | wc -l || echo "0")
    if [ "$LOW_MAINTAIN" -gt 0 ]; then
        print_warning "âš ï¸ MAINTAINABILITY: $LOW_MAINTAIN files with low maintainability index"
        print_status "ğŸ’¡ Run 'uv run radon mi src -s' to identify files needing refactoring"
    fi
fi

# 9. Documentation validation
print_status "ğŸ“š Documentation compliance check..."
DOCSTRING_ISSUES=$(uv run pydocstyle src --count 2>/dev/null || echo "0")
if [ "$DOCSTRING_ISSUES" -gt 0 ]; then
    print_warning "âš ï¸ DOCUMENTATION: $DOCSTRING_ISSUES documentation style issues found"
    print_status "ğŸ’¡ Run 'uv run pydocstyle src' for detailed documentation requirements"
else
    print_success "âœ… Documentation style compliance verified"
fi

# 10. MANDATORY: Type coverage validation
print_status "ğŸ¯ MASTER RULE: Type coverage validation..."
TYPE_COVERAGE=$(uv run mypy --strict src 2>&1 | grep -o "Found [0-9]* error" | grep -o "[0-9]*" || echo "0")
if [ "$TYPE_COVERAGE" -gt 0 ]; then
    print_error "âŒ MASTER RULE VIOLATION: $TYPE_COVERAGE type errors found"
    print_error "ğŸš¨ TYPE SAFETY: 100% type coverage required"
    exit 1
fi

# 11. Result Type Usage Analysis
print_status "ğŸ”„ Defensive programming pattern analysis..."
EXCEPTION_FLOW=$(find src -name "*.py" -exec grep -l "raise.*Exception\|except.*:" {} \; 2>/dev/null | wc -l)
RESULT_USAGE=$(find src -name "*.py" -exec grep -l "Result\[" {} \; 2>/dev/null | wc -l)
if [ "$EXCEPTION_FLOW" -gt 0 ] && [ "$RESULT_USAGE" -eq 0 ]; then
    print_warning "âš ï¸ DEFENSIVE PROGRAMMING: Consider using Result[T, E] types instead of exceptions for flow control"
    print_status "ğŸ’¡ Current: $EXCEPTION_FLOW files use exceptions, $RESULT_USAGE files use Result types"
else
    print_success "âœ… Defensive programming patterns verified ($RESULT_USAGE files using Result types)"
fi

print_header "ğŸ¯ MASTER RULESET ENFORCEMENT SUMMARY"
print_success "âœ… Pydantic Model Compliance: PASSED"
print_success "âœ… Type Safety (MyPy Strict): PASSED"
print_success "âœ… Performance Standards: VERIFIED"
print_success "âœ… Security Scanning: PASSED"
print_success "âœ… Memory Leak Detection: VERIFIED"
print_success "âœ… Code Quality (Ruff): PASSED"
print_header "ğŸš€ ALL MASTER RULESET REQUIREMENTS SATISFIED!"
print_status "ğŸ’¡ Build and comprehensive benchmarks will run in CI pipeline"
