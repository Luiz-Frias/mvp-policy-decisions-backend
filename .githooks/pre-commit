#!/bin/bash
# ğŸ›¡ï¸ MASTER RULESET ENFORCEMENT - Pre-commit Validation Pipeline
# Enforces Defensive Programming & Performance First Principles

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

print_status() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }
print_header() { echo -e "${PURPLE}$1${NC}"; }

print_header "ğŸ›¡ï¸ MASTER RULESET ENFORCEMENT - Pre-commit Validation"
print_status "Enforcing Defensive Programming & Performance First Principles..."

# ---------------------------------------------
# MASTER RULE: Pydantic / Defensive checks
# We want to collect *all* violations and then exit once, rather than aborting
# at the first failure.  Temporarily relax `set -e` and aggregate results.
# ---------------------------------------------

print_status "ğŸ” MASTER RULE: Validating Pydantic model compliance..."

# Disable immediate-exit so we can gather every issue.
set +e
VIOLATIONS=0

# 1ï¸âƒ£  Plain dictionary usage that is not at a SYSTEM_BOUNDARY
DICT_VIOL_FILES=$(find src -name "*.py" -exec grep -l "dict\\[" {} \; 2>/dev/null | xargs grep -L "SYSTEM_BOUNDARY" 2>/dev/null)
if [ -n "$DICT_VIOL_FILES" ]; then
    COUNT=$(echo "$DICT_VIOL_FILES" | wc -l)
    print_error "âŒ MASTER RULE VIOLATION: $COUNT file(s) use plain dictionaries without annotation:" 
    echo "$DICT_VIOL_FILES" | sed 's/^/   â€¢ /'
    VIOLATIONS=$((VIOLATIONS+COUNT))
fi

# 2ï¸âƒ£  Pydantic models missing frozen=True
BASEMODEL_FILES=$(find src -name "*.py" -exec grep -l "class.*BaseModel" {} \; 2>/dev/null)
UNFROZEN_LIST=""
if [ -n "$BASEMODEL_FILES" ]; then
    UNFROZEN_LIST=$(echo "$BASEMODEL_FILES" | xargs grep -L "frozen.*=.*True" 2>/dev/null)
fi
if [ -n "$UNFROZEN_LIST" ]; then
    COUNT=$(echo "$UNFROZEN_LIST" | wc -l)
    print_error "âŒ MASTER RULE VIOLATION: $COUNT model(s) lack frozen=True:" 
    echo "$UNFROZEN_LIST" | sed 's/^/   â€¢ /'
    VIOLATIONS=$((VIOLATIONS+COUNT))
fi

# 3ï¸âƒ£  Public functions missing @beartype
PUBLIC_FUNCS_FILES=$(find src -name "*.py" -exec grep -l "^def [^_]" {} \; 2>/dev/null)
MISSING_BEAR=""
if [ -n "$PUBLIC_FUNCS_FILES" ]; then
    MISSING_BEAR=$(echo "$PUBLIC_FUNCS_FILES" | xargs grep -L "@beartype" 2>/dev/null)
fi
if [ -n "$MISSING_BEAR" ]; then
    COUNT=$(echo "$MISSING_BEAR" | wc -l)
    print_error "âŒ MASTER RULE VIOLATION: $COUNT public function file(s) without @beartype:" 
    echo "$MISSING_BEAR" | sed 's/^/   â€¢ /'
    VIOLATIONS=$((VIOLATIONS+COUNT))
fi

# Re-enable strict mode for the remainder of the script.
set -e

# Final decision for this validation block
if [ "$VIOLATIONS" -gt 0 ]; then
    print_error "ğŸš¨ MASTER RULESET: Fix the violations above before committing"
    exit 1
else
    print_success "âœ… Pydantic model compliance verified"
fi

# 2. MANDATORY: MyPy Strict Mode Enforcement
print_status "ğŸ”’ MASTER RULE: MyPy strict mode validation..."
if ! uv run mypy --strict src; then
    print_error "âŒ MASTER RULE VIOLATION: MyPy strict mode failed"
    print_error "ğŸš¨ TYPE SAFETY: Code must pass mypy --strict without ignores"
    exit 1
fi
print_success "âœ… MyPy strict mode validation passed"

# 3. MANDATORY: Performance Benchmark Validation
print_status "âš¡ MASTER RULE: Performance benchmark validation..."
LONG_FUNCTIONS=$(find src -name "*.py" -exec grep -c "^def \|^    def " {} \; 2>/dev/null | \
    awk -F: '$2 > 10 {print $1}' | wc -l)
BENCHMARK_FILES=$(find tests -name "*benchmark*" -o -name "*perf*" 2>/dev/null | wc -l)
if [ "$LONG_FUNCTIONS" -gt 0 ] && [ "$BENCHMARK_FILES" -eq 0 ]; then
    print_warning "âš ï¸ PERFORMANCE RULE: Functions >10 lines should have performance benchmarks"
    print_status "ğŸ’¡ Create tests/benchmarks/ with pytest-benchmark tests"
fi

# 4. Pre-commit hooks (fast file-level checks)
print_status "ğŸ“ Running pre-commit hooks (auto-fixing issues)..."
# Run pre-commit hooks up to 3 times to handle auto-fixes
max_attempts=3
attempt=1
hooks_passed=false

while [ $attempt -le $max_attempts ]; do
    print_status "ğŸ”§ Pre-commit run attempt $attempt/$max_attempts..."

    if uv run pre-commit run --all-files; then
        hooks_passed=true
        print_success "âœ… All pre-commit hooks passed!"
        break
    else
        if [ $attempt -lt $max_attempts ]; then
            print_warning "âš¡ Pre-commit hooks auto-fixed issues, running again..."

            # Auto-stage any files that were modified by the hooks
            if ! git diff --quiet; then
                print_status "ğŸ“ Auto-staging fixed files for next attempt..."
                git add -A
                print_success "âœ… Modified files staged for retry"
            fi

            # Give a moment for file system to sync
            sleep 1
        fi
    fi

    ((attempt++))
done

if [ "$hooks_passed" != "true" ]; then
    print_error "âŒ Pre-commit hooks failed after $max_attempts attempts!"
    print_error "ğŸš¨ MASTER RULESET VIOLATION: Code quality standards not met"
    print_status "ğŸ’¡ Review the errors above and fix manually, then run:"
    print_status "   cd $(pwd) && pre-commit run --all-files"
    exit 1
fi

# Verify all changes have been staged after pre-commit hooks
if ! git diff --quiet; then
    print_status "ğŸ“ Staging any remaining auto-fixed changes..."
    git add -A
    print_success "âœ… Final auto-fixes applied and staged"
fi

# 5. MANDATORY: Ruff ultra-fast linting
print_status "ğŸš€ MASTER RULE: Ruff ultra-fast linting..."
if ! uv run ruff check src tests; then
    print_error "âŒ MASTER RULE VIOLATION: Ruff linting failed"
    exit 1
fi

# Run ruff format and stage any additional changes
uv run ruff format src tests
if ! git diff --quiet; then
    print_status "ğŸ“ Auto-staging ruff format changes..."
    git add -A
    print_success "âœ… Ruff formatting changes staged"
fi

# Now check that everything is properly formatted
if ! uv run ruff format --check src tests; then
    print_error "âŒ MASTER RULE VIOLATION: Code formatting issues found"
    exit 1
fi
print_success "âœ… Ruff linting and formatting verified"

# 6. MANDATORY: Security scanning
print_status "ğŸ›¡ï¸ MASTER RULE: Security vulnerability scanning..."
uv run bandit -r src -f json -o bandit-report.json >/dev/null 2>&1 || true
HIGH_ISSUES=$(cat bandit-report.json 2>/dev/null | jq -r '.results[] | select(.issue_severity == "HIGH") | .test_name' | wc -l || echo "0")
if [ "$HIGH_ISSUES" -gt 0 ]; then
    print_error "âŒ MASTER RULE VIOLATION: $HIGH_ISSUES high-severity security issues found"
    print_error "ğŸš¨ SECURITY FIRST: Fix all high-severity issues before commit"
    exit 1
fi

# Run safety check on dependencies
if ! uv run safety check --json >/dev/null 2>&1; then
    print_warning "âš ï¸ SECURITY: Dependency vulnerabilities found (check with 'uv run safety check')"
fi

print_success "âœ… Security scanning completed"

# 7. MANDATORY: Memory leak detection in tests
print_status "ğŸ§  MASTER RULE: Memory leak detection..."
if [ -d "tests" ]; then
    # Skip benchmark tests during pre-commit (Wave 3 optimization)
    if ! uv run pytest tests/unit --maxfail=1 -q --tb=no -m "not benchmark" 2>/dev/null; then
        print_error "âŒ MASTER RULE VIOLATION: Unit tests failed"
        print_status "ğŸ’¡ If benchmark plugin is missing, install with: uv add --dev pytest-benchmark"
        exit 1
    fi

    # Check for memory tests
    MEMORY_TESTS=$(find tests -name "*.py" -exec grep -l "tracemalloc\|memray\|memory" {} \; 2>/dev/null | wc -l)
    if [ "$MEMORY_TESTS" -eq 0 ]; then
        print_warning "âš ï¸ PERFORMANCE RULE: No memory tests found"
        print_status "ğŸ’¡ Add memory leak detection tests with tracemalloc"
    fi
fi

# 8. Documentation validation
print_status "ğŸ“š Documentation compliance check..."
if ! uv run pydocstyle src --count >/dev/null 2>&1; then
    print_warning "âš ï¸ Documentation style issues found (run 'uv run pydocstyle src' for details)"
fi

# 9. MANDATORY: Type coverage validation
print_status "ğŸ¯ MASTER RULE: Type coverage validation..."
TYPE_COVERAGE=$(uv run mypy --strict src 2>&1 | grep -o "Found [0-9]* error" | grep -o "[0-9]*" || echo "0")
if [ "$TYPE_COVERAGE" -gt 0 ]; then
    print_error "âŒ MASTER RULE VIOLATION: $TYPE_COVERAGE type errors found"
    print_error "ğŸš¨ TYPE SAFETY: 100% type coverage required"
    exit 1
fi

# 10. MANDATORY: Result Type Usage Check
print_status "ğŸ”„ MASTER RULE: Result type usage validation..."
EXCEPTION_FLOW=$(find src -name "*.py" -exec grep -l "raise.*Exception\|except.*:" {} \; 2>/dev/null | wc -l)
RESULT_USAGE=$(find src -name "*.py" -exec grep -l "Result\[" {} \; 2>/dev/null | wc -l)
if [ "$EXCEPTION_FLOW" -gt 0 ] && [ "$RESULT_USAGE" -eq 0 ]; then
    print_warning "âš ï¸ DEFENSIVE PROGRAMMING: Consider using Result[T, E] types instead of exceptions for flow control"
fi

print_header "ğŸ¯ MASTER RULESET ENFORCEMENT SUMMARY"
print_success "âœ… Pydantic Model Compliance: PASSED"
print_success "âœ… Type Safety (MyPy Strict): PASSED"
print_success "âœ… Performance Standards: VERIFIED"
print_success "âœ… Security Scanning: PASSED"
print_success "âœ… Memory Leak Detection: VERIFIED"
print_success "âœ… Code Quality (Ruff): PASSED"
print_header "ğŸš€ ALL MASTER RULESET REQUIREMENTS SATISFIED!"
print_status "ğŸ’¡ Build and comprehensive benchmarks will run in CI pipeline"
